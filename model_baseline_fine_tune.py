# -*- coding: utf-8 -*-
"""story_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nL1WwP32ajmKFv9_w_NNplDdXpBqAXUV

# Assignment 3 Top-Level Code/Notebook
### Training a language model base on Karpathy's minGPT codebase
"""

# The code below is needed for using Google Colab, so un comment this if that is what you're using
import nltk
nltk.download('punkt')

# Commented out IPython magic to ensure Python compatibility.
# The code below is also needed for using Google Colab
# BEFORE executing this, you must place the mingpt folder supplied in the assignment
# your google drive, within the folder "Colab Notebooks"
#
# It mounts and changes into the folder that contains mingpt, which you must upload to google drive
# So un-comment it if you've uploaded mingpt to your google drive, into the  "Colab Notebooks" folder
"""
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/MyDrive/Colab\ Notebooks/
"""


import torch 
import numpy as np

from nltk.tokenize import sent_tokenize 
from transformers import AutoTokenizer, AutoModelForCausalLM

from pathlib import Path 
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset
from torch.utils.data.dataloader import DataLoader
from mingpt.bpe import BPETokenizer 
from mingpt.utils import set_seed 
set_seed(1234)

genre_dict = {'action': '<action>', 'drama': '<drama>', 'horror': '<horror>', 'sci_fi': '<sci_fi>',  'superhero': '<superhero>', 'thriller': '<thriller>'}

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
genre_tokenizer = AutoTokenizer.from_pretrained("Tejas3/distillbert_110_uncased_movie_genre")    
genre_model = AutoModelForSequenceClassification.from_pretrained("Tejas3/distillbert_110_uncased_movie_genre")

def get_genre(text, tokenizer, model):    
    inputs  = tokenizer(text, return_tensors="pt")    
    with torch.no_grad():
        logits = model(**inputs).logits
    predicted_class_id = logits.argmax().item()
    # all_labels = [model.config.id2label[i] for i in range(6)]
    label = model.config.id2label[predicted_class_id]
    return genre_dict[label]

text = '''It was quite by accident I discovered this incredible invasion of Earth by lifeforms from another planet. As yet, I haven't done anything about it; I can't think of anything to do. I wrote to the Government, and they sent back a pamphlet on the repair and maintenance of frame houses. Anyhow, the whole thing is known; I'm not the first to discover it. Maybe it's even under control. I was sitting in my easy-chair, idly turning the pages of a paperbacked book someone had left on the bus, when I came across the reference that first put me on the trail. For a moment I didn't respond. It took some time for the full import to sink in. After I'd comprehended, it seemed odd I hadn't noticed it right away. The reference was clearly to a nonhuman species of incredible properties, not indigenous to Earth. A species, I hasten to point out, customarily masquerading as ordinary human beings. Their disguise, however, became transparent in the face of the following observations by the author. It was at once obvious the author knew everything. Knew everything--and was taking it in his stride. The line (and I tremble remembering it even now) read: his eyes slowly roved about the room.
Vague chills assailed me. I tried to picture the eyes. Did they roll like dimes? The passage indicated not; they seemed to move through the air, not over the surface. Rather rapidly, apparently. No one in the story was surprised. That's what tipped me off. No sign of amazement at such an outrageous thing. Later the matter was amplified. his eyes moved from person to person.
There it was in a nutshell. The eyes had clearly come apart from the rest of him and were on their own. My heart pounded and my breath choked in my windpipe. I had stumbled on an accidental mention of a totally unfamiliar race. Obviously non-Terrestrial. Yet, to the characters in the book, it was perfectly natural--which suggested they belonged to the same species.
And the author? A slow suspicion burned in my mind. The author was taking it rather too easily in his stride. Evidently, he felt this was quite a usual thing. '''
get_genre(text, genre_tokenizer, genre_model)

"""
Prepare the dataset to train the Language Model (LM)
This implementation splits the sentences and so doesn't create training 
examples that cross sentences.

This code is set so that it uses one of two possible datasets, which were also used in Assignment 1: 
SmallSimpleCorpus.txt or LargerCorpus.txt

Arguments:
            ds_choice: str. "small" or "large". (i.e. selects which of the two datasets)
            split: str. "train" or "test".
            truncation: int. If -1: no truncation on sentences. Otherwise: truncate to this specific length.
""" 

class LanguageModelingDataset(Dataset):
    
    def __init__(self, ds_choice="small", split="train", truncation=-1):
        
        base_path = "./"
        fn = {"small": "SmallSimpleCorpus.txt", "large": "LargerCorpus.txt", "story": "corpus_all.txt"}
        self.ds_choice = ds_choice
        self.truncation = truncation  # int. If -1, then
        text = Path(base_path, fn[ds_choice]).read_text(encoding="utf-8")

        if ds_choice == "large":
            # Remove the newline char in the middle of sentences
            # The "paragraph splitting" newlines appear to be \n\n -- remove the duplications there
            text = text.replace("\n\n", "$$^^$$").replace("\n", " ").replace("$$^^$$", "\n")
        # sentences = sent_tokenize(text)
        sentences = text.split('\n')
        sentences = [s for s in sentences if len(s)>1]
        sentences = ["<BOS> " + get_genre(s, genre_tokenizer, genre_model) + " " + s + " <EOS>" for s in sentences]
        print(sentences)
        # Train / test split
        train, val = train_test_split(sentences, test_size=0.2, shuffle=False)
        if split == "train":
            raw_data = train 
        else:
            raw_data = val 

        # Tokenize
        # self.tokenizer = BPETokenizer()
                
        # model_name = "pranavpsv/genre-story-generator-v2"
        model_name = "pranavpsv/gpt2-genre-story-generator"
        # model_name = "openai-gpt"
        # model_name = "pranavpsv/gpt2-story-gen"
        # model_name = 'gpt2'

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.data = []  # List of 1-d pytorch tensor
        for sent in raw_data:
            # print(self.tokenizer(sent))
            tokenized = torch.from_numpy(np.array(self.tokenizer(sent)["input_ids"])).view(-1)  # pytorch tensor
            if truncation >= 0:
                self.data.append(tokenized[:truncation])
            else:
                self.data.append(tokenized)

        # Count some items
        self.max_sentence_length = np.max([len(d) for d in self.data])

    def __len__(self):
        return len(self.data)

    def get_vocab_size(self):
        """
        We have to set this to the max vocab size (i.e., that decided by the BPE tokenizer), 
        but actually, only a small number of vocab is used, especially for the small text. 
        """
        return 50257

    def __getitem__(self, idx):
        """
        The output should be a tuple x and y, both as pytorch tensors.
        Please refer to the `run()` method in the mingpt/trainer.py script for 
        how the x and y are going to be used.
        """
        x = self.data[idx][:-1].long()
        y = self.data[idx][1:].long()
        return (x, y)

    def get_block_size(self):
        """
        block_size is the size at which lines are truncated to ensure they are equal-length.
        """
        return self.max_sentence_length
    
# Instantiate the Training Dataset
# train_dataset = LanguageModelingDataset(ds_choice="small", split="train")  # use this for the short corpus
train_dataset = LanguageModelingDataset(ds_choice="story", split="train", truncation=512) #use this for long

# Instantiate a Validation Dataset (this is only really needed for the fine-tune task, not the LM task)
# val_dataset = LanguageModelingDataset(ds_choice="small", split="validation")
val_dataset = LanguageModelingDataset(ds_choice="story", split="validation", truncation=512)


model_name = "pranavpsv/gpt2-genre-story-generator"

tokenizer = AutoTokenizer.from_pretrained(model_name)

tokenizer("<BOS> <horror> Burger Queen flew Billy and his Bronx coworkers to its training center.")

tokenizer

tokenizer("<sci_fi>")

train_dataset[0]

def lm_collate_fn(batch, device='cuda'):
    x = [item[0] for item in batch]  # List (len B) of varying lengths
    y = [item[1] for item in batch]  # List (len B) of the same lengths as x
    maxlen = max([len(s) for s in x])

    padded_x, padded_y = [], []
    for sx, sy in zip(x, y):
        padded_x.append(torch.cat([sx, torch.ones(maxlen - len(sx))]))
        padded_y.append(torch.cat([sy, torch.ones(maxlen - len(sy))]))
    return torch.stack(padded_x).long().to(device), torch.stack(padded_y).long().to(device)

# Print out an example of the data - this is processed more once it reaches lm_collate_fn (above)
x,y = train_dataset[5]
print(x, y)
print("X: ",train_dataset.tokenizer.decode(x))
print("Y: ",train_dataset.tokenizer.decode(y))

from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "pranavpsv/gpt2-genre-story-generator"
# model_name = "openai-gpt"
# model_name = "pranavpsv/gpt2-story-gen"
# model_name = 'gpt2'

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

from torch.utils.data import DataLoader
from transformers import DistilBertForSequenceClassification, AdamW

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

model.to(device)

model_parameters = filter(lambda p: p.requires_grad, model.parameters())
params = sum([np.prod(p.size()) for p in model_parameters])
print(params)

for param in model.parameters():
    param.requires_grad = False

model.lm_head.weight.requires_grad = True

model_parameters = filter(lambda p: p.requires_grad, model.parameters())
params = sum([np.prod(p.size()) for p in model_parameters])
print(params)

train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, pin_memory=False)
val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, pin_memory=False)

model.train()

optim = AdamW(model.parameters(), lr=1e-6)
train_loss = []
val_loss = []

for epoch in range(100):
    epoch_loss = []
    for batch in train_loader:
        optim.zero_grad()
        input_ids = batch[0].to(device)
        # attention_mask = batch['attention_mask'].to(device)
        labels = batch[1].to(device)
        outputs = model(input_ids,  labels=labels)
        loss = outputs[0]
        loss.backward()
        optim.step()
        epoch_loss.append(loss.item())         
    val_epoch_loss = []
    for batch in val_loader:
        optim.zero_grad()
        input_ids = batch[0].to(device)
        # attention_mask = batch['attention_mask'].to(device)
        labels = batch[1].to(device)
        outputs = model(input_ids,  labels=labels)
        loss = outputs[0]        
        val_epoch_loss.append(loss.item())               

    print("Train loss: {}".format(np.mean(epoch_loss)))
    print("Val loss: {}".format(np.mean(val_epoch_loss)))

    train_loss.append(np.mean(epoch_loss))
    val_loss.append(np.mean(val_epoch_loss))
    if len(val_loss)>1 and (val_loss[-1] < np.min(val_loss[0:-1])):
        torch.save(model.state_dict(), "E:/projects/courses/nlp/saved_models/gpt2_story_best_model")


model.eval()

import matplotlib.pyplot as plt
plt.figure()
plt.xlabel("Epoch")
plt.ylabel("Loss")
x = np.array(list(range(len(train_loss))))+1
plt.plot(x,train_loss, label="Training")
plt.plot(x,val_loss, label="Validation")
plt.scatter(x,train_loss)
plt.scatter(x,val_loss)

plt.grid()
plt.legend()
plt.show()
# plt.savefig("fig4.png",dpi=300)

# Use the trained language model to predict a sequence of words following a few words
encoded_prompt = train_dataset.tokenizer("<BOS> <drama> We need to lose weight" , return_tensors="pt").to(device)
generated_sequence = model.generate(**encoded_prompt, max_length=400,  do_sample=True)
train_dataset.tokenizer.decode(generated_sequence[0])

import random
begin = ['A long time ago and far,', 'Once upon a time,']
skeletons = ["","","","","But", "After a while,", "However,", "Then,", "So,", "In fact,", "Moreover", "In contrast,", "Consequently,", "Because of", "Furthermore"]
end = ['Finally, ']

# genre = ''
# genre = "<action>"
# genre = "<sci_fi>"
# genre = "<horror>"
genre = "<drama>"

text = "<BOS> " + genre + " " + random.choice(begin)
flag = True
while flag:
    with torch.no_grad():
        # output = model.generate(**inputs, max_length=200, do_sample=True)

        print(text)
        print("---------------------")
        encoded_prompt = train_dataset.tokenizer(text , return_tensors="pt").to(device)
        generated_sequence = model.generate(**encoded_prompt, max_length=400,  do_sample=True)
        new_generated = train_dataset.tokenizer.decode(generated_sequence[0])[len(text):]
        dot_point = new_generated.find('.')
        text = text +  new_generated[:dot_point+1] + ' '
        for w in end:
            if w in text:
                flag = False
        if flag==True:
            if len(text.split(" "))>150:
                text = text + random.choice(end)
            else:
                text = text + random.choice(skeletons)
